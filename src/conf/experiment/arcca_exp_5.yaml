# @package _global_

# Use the pre-trained fast_joint encoder
# Train on OntoNotes and i2b2

defaults:
  - override /datasets: joint_exp_5
  - override /trainer: train.yaml
  # The pre-trained doc_encoder model is downloaded and cached locally
  - override /model/doc_encoder/transformer: longformer_joint_arcca_local

trainer:
  log_frequency: 250
  # Stop when no improvement in 10 evaluations 
  patience: 10
  max_evals: 100
  eval_per_k_steps: 941 # if eval_per_k_steps == # of doc, it means eval per epoch, then max_evals is the # of epoch

use_wandb: False

# There are two cases where we don't resume training:
# (a) The dev performance has not improved for the allowed patience parameter number of evaluations.
# (b) Number of gradient updates is already >= Total training steps.


datasets:
  ontonotes:
    num_train_docs: 500

model:
  doc_encoder:
    add_speaker_tokens: True
    finetune: False
