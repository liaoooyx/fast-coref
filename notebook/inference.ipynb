{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/yuxiang/liao/workspace/fast-coref/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from inference.tokenize_doc import basic_tokenize_doc, tokenize_and_segment_doc\n",
    "from model.entity_ranking_model import EntityRankingModel\n",
    "from model.utils import action_sequences_to_clusters\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, model_path, encoder_name=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load model\n",
    "        checkpoint = torch.load(path.join(model_path, \"model.pth\"), map_location=self.device)\n",
    "        self.config = OmegaConf.create(checkpoint[\"config\"])\n",
    "        if encoder_name is not None:\n",
    "            self.config.model.doc_encoder.transformer.model_str = encoder_name\n",
    "        self.model = EntityRankingModel(self.config.model, self.config.trainer)\n",
    "        self._load_model(checkpoint, model_path, encoder_name=encoder_name)\n",
    "\n",
    "        self.max_segment_len = self.config.model.doc_encoder.transformer.max_segment_len\n",
    "        self.tokenizer = self.model.mention_proposer.doc_encoder.tokenizer\n",
    "\n",
    "    def _load_model(self, checkpoint, model_path, encoder_name=None):\n",
    "        self.model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "\n",
    "        if self.config.model.doc_encoder.finetune:\n",
    "            # Load the document encoder params if encoder is finetuned\n",
    "            if encoder_name is None:\n",
    "                doc_encoder_dir = path.join(model_path, self.config.paths.doc_encoder_dirname)\n",
    "                # else:\n",
    "                # \tdoc_encoder_dir = encoder_name\n",
    "                # Load the encoder\n",
    "                self.model.mention_proposer.doc_encoder.lm_encoder = AutoModel.from_pretrained(pretrained_model_name_or_path=doc_encoder_dir)\n",
    "                self.model.mention_proposer.doc_encoder.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=doc_encoder_dir)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                self.model.cuda()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perform_coreference(self, document):\n",
    "        if isinstance(document, list):\n",
    "            # Document is already tokenized\n",
    "            tokenized_doc = tokenize_and_segment_doc(document, self.tokenizer, max_segment_len=self.max_segment_len)\n",
    "        elif isinstance(document, str):\n",
    "            # Raw document string. First perform basic tokenization before further tokenization.\n",
    "            import spacy\n",
    "\n",
    "            basic_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "            basic_tokenized_doc = basic_tokenize_doc(document, basic_tokenizer)\n",
    "            tokenized_doc = tokenize_and_segment_doc(\n",
    "                basic_tokenized_doc,\n",
    "                self.tokenizer,\n",
    "                max_segment_len=self.max_segment_len,\n",
    "            )\n",
    "        elif isinstance(document, dict):\n",
    "            tokenized_doc = document\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        extra_output_dict = {}\n",
    "        pred_mentions, _, _, pred_actions = self.model(tokenized_doc, extra_output=extra_output_dict)\n",
    "        idx_clusters = action_sequences_to_clusters(pred_actions, pred_mentions)\n",
    "\n",
    "        subtoken_map = tokenized_doc[\"subtoken_map\"]\n",
    "        orig_tokens = tokenized_doc[\"orig_tokens\"]\n",
    "        clusters = []\n",
    "        for idx_cluster in idx_clusters:\n",
    "            cur_cluster = []\n",
    "            for ment_start, ment_end in idx_cluster:\n",
    "                cur_cluster.append(\n",
    "                    (\n",
    "                        (ment_start, ment_end),\n",
    "                        \" \".join(orig_tokens[subtoken_map[ment_start] : subtoken_map[ment_end] + 1]),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            clusters.append(cur_cluster)\n",
    "\n",
    "        return {\n",
    "            \"tokenized_doc\": tokenized_doc,\n",
    "            \"clusters\": clusters,\n",
    "            \"subtoken_idx_clusters\": idx_clusters,\n",
    "            \"actions\": pred_actions,\n",
    "            \"mentions\": pred_mentions,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[((0, 3), 'A calcific density'), ((15, 18), 'The calcific density'), ((26, 29), 'The calcific density'), ((41, 44), 'The calcific density')]]\n"
     ]
    }
   ],
   "source": [
    "model_str = \"/home/yuxiang/liao/resources/downloaded_models/coref_model_9b02_25_4/best\"  # exp11\n",
    "encoder_name = \"/home/yuxiang/liao/resources/downloaded_models/longformer_coreference_joint\"\n",
    "# model = Inference(model_str)\n",
    "model = Inference(model_str, encoder_name)\n",
    "\n",
    "# doc = \" \".join(open(\"/home/shtoshni/Research/coref_resources/data/ccarol/doc.txt\").readlines())\n",
    "doc = [[\"A\", \"calcific\", \"density\", \"is\", \"seen\", \"projecting\", \"at\", \"the\", \"left\", \"lung\", \"base\", \"laterally\", \".\"], [\"The\", \"calcific\", \"density\", \"may\", \"reflect\", \"a\", \"granuloma\", \".\"], [\"The\", \"calcific\", \"density\", \"may\", \"reflect\", \"a\", \"sclerotic\", \"finding\", \"within\", \"the\", \"rib\", \".\"], [\"The\", \"calcific\", \"density\", \"may\", \"reflect\", \"an\", \"object\", \"external\", \"to\", \"the\", \"patient\", \".\"]]\n",
    "output_dict = model.perform_coreference(doc)\n",
    "print(output_dict[\"clusters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [i for s in doc for i in s]\n",
    "output_dict[\"tokenized_doc\"][\"sentence_map\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/home/yuxiang/liao/workspace/arrg_sentgen/outputs/interpret_cxr\"\n",
    "\n",
    "with open(os.path.join(input_dir, \"raw.json\"), \"r\") as f:\n",
    "    raw_docs = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_key': 'train#0#impression',\n",
       " 'valid_key': 'CheXpert#data/chexpert-public/train/patient32815/study11/view1_frontal.jpg',\n",
       " 'sentences': [['1.DECREASED',\n",
       "   'BIBASILAR',\n",
       "   'PARENCHYMAL',\n",
       "   'OPACITIES',\n",
       "   ',',\n",
       "   'NOW',\n",
       "   'MINIMAL',\n",
       "   '.'],\n",
       "  ['STABLE', 'SMALL', 'LEFT', 'PLEURAL', 'EFFUSION', '.'],\n",
       "  ['2',\n",
       "   '.',\n",
       "   'FEEDING',\n",
       "   'TUBE',\n",
       "   'AND',\n",
       "   'STERNAL',\n",
       "   'PLATES',\n",
       "   'AGAIN',\n",
       "   'SEEN',\n",
       "   '.']]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(input_dir, \"llm_split_sents_1_of_3.json\"), \"r\") as f:\n",
    "    split_docs = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_key': 'train#0#impression',\n",
       " 'sent_idx': 1,\n",
       " 'original_sent': 'STABLE SMALL LEFT PLEURAL EFFUSION .',\n",
       " 'sent_splits': ['Stable small left pleural effusion.']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "input_dir = \"/home/yuxiang/liao/workspace/arrg_data_processing/outputs/interpret_sents/raw.json\"\n",
    "\n",
    "docs_dict = defaultdict(list)\n",
    "with open(input_dir, \"r\") as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        data_split_name, data_row_idx, section_name, doc_sent_id = doc[\"doc_key\"].split(\"#\")\n",
    "        doc_sent_idx, split_sent_idx = doc_sent_id.split(\"@\")\n",
    "        doc_key = f\"{data_split_name}#{data_row_idx}#{section_name}\"\n",
    "        doc[\"doc_sent_idx\"] = doc_sent_idx\n",
    "        doc[\"split_sent_idx\"] = split_sent_idx\n",
    "        docs_dict[doc_key].append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "734635"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_key': 'train#0#impression#0@0',\n",
       "  'sentences': [['Decreased',\n",
       "    'bibasilar',\n",
       "    'parenchymal',\n",
       "    'opacities',\n",
       "    'are',\n",
       "    'seen',\n",
       "    '.']],\n",
       "  'tok_indices': [[[0, 9],\n",
       "    [10, 19],\n",
       "    [20, 31],\n",
       "    [32, 41],\n",
       "    [42, 45],\n",
       "    [46, 50],\n",
       "    [50, 51]]],\n",
       "  'raw_sent': 'Decreased bibasilar parenchymal opacities are seen.',\n",
       "  'doc_sent_idx': '0',\n",
       "  'split_sent_idx': '0'},\n",
       " {'doc_key': 'train#0#impression#0@1',\n",
       "  'sentences': [['The',\n",
       "    'bibasilar',\n",
       "    'parenchymal',\n",
       "    'opacities',\n",
       "    'are',\n",
       "    'now',\n",
       "    'minimal',\n",
       "    '.']],\n",
       "  'tok_indices': [[[0, 3],\n",
       "    [4, 13],\n",
       "    [14, 25],\n",
       "    [26, 35],\n",
       "    [36, 39],\n",
       "    [40, 43],\n",
       "    [44, 51],\n",
       "    [51, 52]]],\n",
       "  'raw_sent': 'The bibasilar parenchymal opacities are now minimal.',\n",
       "  'doc_sent_idx': '0',\n",
       "  'split_sent_idx': '1'},\n",
       " {'doc_key': 'train#0#impression#1@0',\n",
       "  'sentences': [['Stable', 'small', 'left', 'pleural', 'effusion', '.']],\n",
       "  'tok_indices': [[[0, 6], [7, 12], [13, 17], [18, 25], [26, 34], [34, 35]]],\n",
       "  'raw_sent': 'Stable small left pleural effusion.',\n",
       "  'doc_sent_idx': '1',\n",
       "  'split_sent_idx': '0'},\n",
       " {'doc_key': 'train#0#impression#2@0',\n",
       "  'sentences': [['Feeding', 'tube', 'is', 'again', 'seen', '.']],\n",
       "  'tok_indices': [[[0, 7], [8, 12], [13, 15], [16, 21], [22, 26], [26, 27]]],\n",
       "  'raw_sent': 'Feeding tube is again seen.',\n",
       "  'doc_sent_idx': '2',\n",
       "  'split_sent_idx': '0'},\n",
       " {'doc_key': 'train#0#impression#2@1',\n",
       "  'sentences': [['Sternal', 'plates', 'are', 'again', 'seen', '.']],\n",
       "  'tok_indices': [[[0, 7], [8, 14], [15, 18], [19, 24], [25, 29], [29, 30]]],\n",
       "  'raw_sent': 'Sternal plates are again seen.',\n",
       "  'doc_sent_idx': '2',\n",
       "  'split_sent_idx': '1'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_dict[\"train#0#impression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_key': 'train#0#impression', 'split_sents': [['Decreased', 'bibasilar', 'parenchymal', 'opacities', 'are', 'seen', '.'], ['The', 'bibasilar', 'parenchymal', 'opacities', 'are', 'now', 'minimal', '.'], ['Stable', 'small', 'left', 'pleural', 'effusion', '.'], ['Feeding', 'tube', 'is', 'again', 'seen', '.'], ['Sternal', 'plates', 'are', 'again', 'seen', '.']], 'sent_idx_split_idx': [(0, 0), (0, 1), (1, 0), (2, 0), (2, 1)]}\n"
     ]
    }
   ],
   "source": [
    "doc_key = \"train#0#impression\"\n",
    "out_dict = {\"doc_key\": doc_key, \"split_sents\": [], \"sent_idx_split_idx\": []}\n",
    "for doc in docs_dict[\"train#0#impression\"]:\n",
    "    out_dict[\"split_sents\"] += doc[\"sentences\"]\n",
    "    out_dict[\"sent_idx_split_idx\"].append((int(doc[\"doc_sent_idx\"]), int(doc[\"split_sent_idx\"])))\n",
    "\n",
    "print(out_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrg_coref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
