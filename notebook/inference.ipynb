{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/yuxiang/liao/workspace/fast-coref/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from inference.tokenize_doc import basic_tokenize_doc, tokenize_and_segment_doc\n",
    "from model.entity_ranking_model import EntityRankingModel\n",
    "from model.utils import action_sequences_to_clusters\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, model_path, encoder_name=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load model\n",
    "        checkpoint = torch.load(path.join(model_path, \"model.pth\"), map_location=self.device)\n",
    "        self.config = OmegaConf.create(checkpoint[\"config\"])\n",
    "        if encoder_name is not None:\n",
    "            self.config.model.doc_encoder.transformer.model_str = encoder_name\n",
    "        self.model = EntityRankingModel(self.config.model, self.config.trainer)\n",
    "        self._load_model(checkpoint, model_path, encoder_name=encoder_name)\n",
    "\n",
    "        self.max_segment_len = self.config.model.doc_encoder.transformer.max_segment_len\n",
    "        self.tokenizer = self.model.mention_proposer.doc_encoder.tokenizer\n",
    "\n",
    "    def _load_model(self, checkpoint, model_path, encoder_name=None):\n",
    "        self.model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "\n",
    "        if self.config.model.doc_encoder.finetune:\n",
    "            # Load the document encoder params if encoder is finetuned\n",
    "            if encoder_name is None:\n",
    "                doc_encoder_dir = path.join(model_path, self.config.paths.doc_encoder_dirname)\n",
    "                # else:\n",
    "                # \tdoc_encoder_dir = encoder_name\n",
    "                # Load the encoder\n",
    "                self.model.mention_proposer.doc_encoder.lm_encoder = AutoModel.from_pretrained(pretrained_model_name_or_path=doc_encoder_dir)\n",
    "                self.model.mention_proposer.doc_encoder.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=doc_encoder_dir)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                self.model.cuda()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perform_coreference(self, document):\n",
    "        if isinstance(document, list):\n",
    "            # Document is already tokenized\n",
    "            tokenized_doc = tokenize_and_segment_doc(document, self.tokenizer, max_segment_len=self.max_segment_len)\n",
    "        elif isinstance(document, str):\n",
    "            # Raw document string. First perform basic tokenization before further tokenization.\n",
    "            import spacy\n",
    "\n",
    "            basic_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "            basic_tokenized_doc = basic_tokenize_doc(document, basic_tokenizer)\n",
    "            tokenized_doc = tokenize_and_segment_doc(\n",
    "                basic_tokenized_doc,\n",
    "                self.tokenizer,\n",
    "                max_segment_len=self.max_segment_len,\n",
    "            )\n",
    "        elif isinstance(document, dict):\n",
    "            tokenized_doc = document\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        extra_output_dict = {}\n",
    "        pred_mentions, _, _, pred_actions = self.model(tokenized_doc, extra_output=extra_output_dict)\n",
    "        idx_clusters = action_sequences_to_clusters(pred_actions, pred_mentions)\n",
    "\n",
    "        subtoken_map = tokenized_doc[\"subtoken_map\"]\n",
    "        orig_tokens = tokenized_doc[\"orig_tokens\"]\n",
    "        clusters = []\n",
    "        for idx_cluster in idx_clusters:\n",
    "            cur_cluster = []\n",
    "            for ment_start, ment_end in idx_cluster:\n",
    "                cur_cluster.append(\n",
    "                    (\n",
    "                        (ment_start, ment_end),\n",
    "                        \" \".join(orig_tokens[subtoken_map[ment_start] : subtoken_map[ment_end] + 1]),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            clusters.append(cur_cluster)\n",
    "\n",
    "        return {\n",
    "            \"tokenized_doc\": tokenized_doc,\n",
    "            \"clusters\": clusters,\n",
    "            \"subtoken_idx_clusters\": idx_clusters,\n",
    "            \"actions\": pred_actions,\n",
    "            \"mentions\": pred_mentions,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[((0, 3), 'A calcific density'), ((15, 18), 'The calcific density'), ((26, 29), 'The calcific density'), ((41, 44), 'The calcific density')]]\n"
     ]
    }
   ],
   "source": [
    "model_str = \"/home/yuxiang/liao/resources/downloaded_models/coref_model_9b02_25_4/best\"  # exp11\n",
    "encoder_name = \"/home/yuxiang/liao/resources/downloaded_models/longformer_coreference_joint\"\n",
    "# model = Inference(model_str)\n",
    "model = Inference(model_str, encoder_name)\n",
    "\n",
    "# doc = \" \".join(open(\"/home/shtoshni/Research/coref_resources/data/ccarol/doc.txt\").readlines())\n",
    "doc = [[\"A\", \"calcific\", \"density\", \"is\", \"seen\", \"projecting\", \"at\", \"the\", \"left\", \"lung\", \"base\", \"laterally\", \".\"], [\"The\", \"calcific\", \"density\", \"may\", \"reflect\", \"a\", \"granuloma\", \".\"], [\"The\", \"calcific\", \"density\", \"may\", \"reflect\", \"a\", \"sclerotic\", \"finding\", \"within\", \"the\", \"rib\", \".\"], [\"The\", \"calcific\", \"density\", \"may\", \"reflect\", \"an\", \"object\", \"external\", \"to\", \"the\", \"patient\", \".\"]]\n",
    "output_dict = model.perform_coreference(doc)\n",
    "print(output_dict[\"clusters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [i for s in doc for i in s]\n",
    "output_dict[\"tokenized_doc\"][\"sentence_map\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/home/yuxiang/liao/workspace/arrg_sentgen/outputs/interpret_cxr\"\n",
    "\n",
    "with open(os.path.join(input_dir, \"raw.json\"), \"r\") as f:\n",
    "    raw_docs = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_key': 'train#0#impression',\n",
       " 'valid_key': 'CheXpert#data/chexpert-public/train/patient32815/study11/view1_frontal.jpg',\n",
       " 'sentences': [['1.DECREASED',\n",
       "   'BIBASILAR',\n",
       "   'PARENCHYMAL',\n",
       "   'OPACITIES',\n",
       "   ',',\n",
       "   'NOW',\n",
       "   'MINIMAL',\n",
       "   '.'],\n",
       "  ['STABLE', 'SMALL', 'LEFT', 'PLEURAL', 'EFFUSION', '.'],\n",
       "  ['2',\n",
       "   '.',\n",
       "   'FEEDING',\n",
       "   'TUBE',\n",
       "   'AND',\n",
       "   'STERNAL',\n",
       "   'PLATES',\n",
       "   'AGAIN',\n",
       "   'SEEN',\n",
       "   '.']]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(input_dir, \"llm_sent_splits_1_of_3.json\"), \"r\") as f:\n",
    "    split_docs = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_key': 'train#0#impression',\n",
       " 'sent_idx': 1,\n",
       " 'original_sent': 'STABLE SMALL LEFT PLEURAL EFFUSION .',\n",
       " 'sent_splits': ['Stable small left pleural effusion.']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrg_coref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
